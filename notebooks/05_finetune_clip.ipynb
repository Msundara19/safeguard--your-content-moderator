{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e036b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cpu\":\n",
    "    print(\"âš ï¸  WARNING: Training on CPU will be VERY slow!\")\n",
    "    print(\"   Consider using Google Colab with free GPU\")\n",
    "    print(\"   Continuing anyway...\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results/training', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd86b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentModerationDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, max_length=77):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(row['image_path']).convert('RGB')\n",
    "        except Exception as e:\n",
    "            # Return a blank image on error\n",
    "            image = Image.new('RGB', (224, 224), color='white')\n",
    "        \n",
    "        # Label: 0 = safe, 1 = unsafe\n",
    "        label = 1 if row['label'] == 'unsafe' else 0\n",
    "        \n",
    "        # Process image (processor handles resizing, normalization)\n",
    "        inputs = self.processor(\n",
    "            images=image,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('datasets/metadata/combined_dataset.csv')\n",
    "\n",
    "train_df = df[df['split'] == 'train'].copy()\n",
    "val_df = df[df['split'] == 'val'].copy()\n",
    "test_df = df[df['split'] == 'test'].copy()\n",
    "\n",
    "print(f\"Train: {len(train_df)} | Val: {len(val_df)} | Test: {len(test_df)}\")\n",
    "print(f\"\\nTrain label distribution:\")\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d872e872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CLIP model and processor\n",
    "model_name = \"openai/clip-vit-base-patch32\"\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "clip_model = CLIPModel.from_pretrained(model_name)\n",
    "\n",
    "# Create classification head\n",
    "class CLIPClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.clip = clip_model\n",
    "        self.classifier = nn.Linear(512, num_classes)  # CLIP base has 512-dim embeddings\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, pixel_values):\n",
    "        # Get image features from CLIP\n",
    "        outputs = self.clip.get_image_features(pixel_values=pixel_values)\n",
    "        # Apply dropout\n",
    "        outputs = self.dropout(outputs)\n",
    "        # Classify\n",
    "        logits = self.classifier(outputs)\n",
    "        return logits\n",
    "\n",
    "model = CLIPClassifier(clip_model)\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"âœ… Model initialized\")\n",
    "print(f\"   Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"   Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ContentModerationDataset(train_df, processor)\n",
    "val_dataset = ContentModerationDataset(val_df, processor)\n",
    "test_dataset = ContentModerationDataset(test_df, processor)\n",
    "\n",
    "print(f\"\\nâœ… Datasets created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3445d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "BATCH_SIZE = 32  # Reduce if out of memory\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 1e-5  # Small LR for fine-tuning\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Total training batches: {len(train_loader)}\")\n",
    "print(f\"  Total validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\")\n",
    "    for batch in pbar:\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(pixel_values)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Validation\"):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(pixel_values)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100. * correct / total\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_model_path = 'models/clip_finetuned_best.pt'\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "        }, best_model_path)\n",
    "        print(f\"  âœ… New best model saved! Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb4b912",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "# Loss plot\n",
    "ax1 = axes[0]\n",
    "ax1.plot(epochs_range, history['train_loss'], 'b-', label='Train Loss')\n",
    "ax1.plot(epochs_range, history['val_loss'], 'r-', label='Val Loss')\n",
    "ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2 = axes[1]\n",
    "ax2.plot(epochs_range, history['train_acc'], 'b-', label='Train Acc')\n",
    "ax2.plot(epochs_range, history['val_acc'], 'r-', label='Val Acc')\n",
    "ax2.set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy (%)')\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/training/training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Saved training history plot\")\n",
    "\n",
    "# Save history\n",
    "with open('results/training/history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13034bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(best_model_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(pixel_values)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Convert to numpy\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "# Calculate metrics\n",
    "test_accuracy = accuracy_score(all_labels, all_preds)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINE-TUNED MODEL - TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Classification report\n",
    "target_names = ['Safe', 'Unsafe']\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=target_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"              Safe  Unsafe\")\n",
    "print(f\"Actual Safe   {cm[0][0]:4d}  {cm[0][1]:4d}\")\n",
    "print(f\"       Unsafe {cm[1][0]:4d}  {cm[1][1]:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847f681f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline results\n",
    "with open('results/clip_baseline_metrics.json', 'r') as f:\n",
    "    baseline_results = json.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BASELINE vs FINE-TUNED COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBaseline (Zero-Shot):\")\n",
    "print(f\"  Accuracy: {baseline_results['accuracy']*100:.2f}%\")\n",
    "print(f\"  F1 Score: {baseline_results['f1_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nFine-Tuned:\")\n",
    "print(f\"  Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "improvement = (test_accuracy - baseline_results['accuracy']) * 100\n",
    "print(f\"\\nðŸ“ˆ Improvement: +{improvement:.2f} percentage points\")\n",
    "print(f\"   ({improvement/baseline_results['accuracy']*100:.1f}% relative improvement)\")\n",
    "\n",
    "# Save fine-tuned results\n",
    "finetuned_results = {\n",
    "    'model': 'CLIP Fine-Tuned',\n",
    "    'test_accuracy': float(test_accuracy),\n",
    "    'best_val_accuracy': float(best_val_acc),\n",
    "    'epochs_trained': EPOCHS,\n",
    "    'baseline_accuracy': baseline_results['accuracy'],\n",
    "    'improvement': float(improvement),\n",
    "}\n",
    "\n",
    "with open('results/clip_finetuned_metrics.json', 'w') as f:\n",
    "    json.dump(finetuned_results, f, indent=2)\n",
    "\n",
    "print(\"\\nâœ… Results saved to results/clip_finetuned_metrics.json\")\n",
    "print(\"âœ… Model saved to models/clip_finetuned_best.pt\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
