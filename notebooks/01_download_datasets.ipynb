{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c3fecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Create directories\n",
    "os.makedirs('datasets/raw', exist_ok=True)\n",
    "os.makedirs('datasets/processed', exist_ok=True)\n",
    "os.makedirs('datasets/metadata', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ab2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_kaggle_dataset(dataset_path, output_name):\n",
    "    \"\"\"\n",
    "    Download and extract Kaggle dataset\n",
    "    \"\"\"\n",
    "    output_dir = f'datasets/raw/{output_name}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Downloading: {dataset_path}\")\n",
    "    print(f\"Output: {output_dir}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    result = subprocess.run(\n",
    "        ['kaggle', 'datasets', 'download', '-d', dataset_path, '-p', output_dir],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(f\"✅ Downloaded successfully\")\n",
    "        \n",
    "        # Unzip all zip files\n",
    "        zip_files = list(Path(output_dir).glob('*.zip'))\n",
    "        for zip_file in zip_files:\n",
    "            print(f\"Extracting {zip_file.name}...\")\n",
    "            try:\n",
    "                with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(output_dir)\n",
    "                zip_file.unlink()\n",
    "                print(f\"✅ Extracted and removed zip\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Error extracting: {e}\")\n",
    "        \n",
    "        # Show contents\n",
    "        print(f\"\\nContents of {output_dir}:\")\n",
    "        items = list(Path(output_dir).iterdir())\n",
    "        for item in items[:10]:  # Show first 10 items\n",
    "            size = item.stat().st_size / (1024*1024)  # MB\n",
    "            print(f\"  - {item.name} ({size:.2f} MB)\")\n",
    "        if len(items) > 10:\n",
    "            print(f\"  ... and {len(items)-10} more items\")\n",
    "        \n",
    "        return True\n",
    "    else:\n",
    "        print(f\"❌ Error downloading: {result.stderr}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06dd27d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Downloading: jjeevanprakash/nsfw-detection\n",
      "Output: datasets/raw/nsfw_detection\n",
      "============================================================\n",
      "✅ Downloaded successfully\n",
      "Extracting nsfw-detection.zip...\n",
      "✅ Extracted and removed zip\n",
      "\n",
      "Contents of datasets/raw/nsfw_detection:\n",
      "  - out (0.00 MB)\n",
      "\n",
      "============================================================\n",
      "Downloading: kartikeybartwal/graphical-violence-and-safe-images-dataset\n",
      "Output: datasets/raw/violence_safe\n",
      "============================================================\n",
      "✅ Downloaded successfully\n",
      "Extracting graphical-violence-and-safe-images-dataset.zip...\n",
      "✅ Extracted and removed zip\n",
      "\n",
      "Contents of datasets/raw/violence_safe:\n",
      "  - Graphically Violent Images (0.00 MB)\n",
      "  - Graphically Safe Images (0.07 MB)\n",
      "\n",
      "============================================================\n",
      "Downloading: thedevastator/hate-speech-and-offensive-language-detection\n",
      "Output: datasets/raw/hate_speech\n",
      "============================================================\n",
      "✅ Downloaded successfully\n",
      "Extracting hate-speech-and-offensive-language-detection.zip...\n",
      "✅ Extracted and removed zip\n",
      "\n",
      "Contents of datasets/raw/hate_speech:\n",
      "  - train.csv (2.30 MB)\n",
      "\n",
      "============================================================\n",
      "DOWNLOAD SUMMARY\n",
      "============================================================\n",
      "nsfw_detection: ✅ Success\n",
      "violence_safe: ✅ Success\n",
      "hate_speech: ✅ Success\n"
     ]
    }
   ],
   "source": [
    "# Download the main datasets we identified\n",
    "datasets = {\n",
    "    'nsfw_detection': 'jjeevanprakash/nsfw-detection',\n",
    "    'violence_safe': 'kartikeybartwal/graphical-violence-and-safe-images-dataset',\n",
    "    'hate_speech': 'thedevastator/hate-speech-and-offensive-language-detection',\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, kaggle_path in datasets.items():\n",
    "    success = download_kaggle_dataset(kaggle_path, name)\n",
    "    results[name] = success\n",
    "    \n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOWNLOAD SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for name, success in results.items():\n",
    "    status = \"✅ Success\" if success else \"❌ Failed\"\n",
    "    print(f\"{name}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2a88e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring downloaded datasets...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Dataset: nsfw_detection\n",
      "============================================================\n",
      "Total items: 27741\n",
      "File types: {'.jpg': 27532, '.png': 199}\n",
      "Subdirectories: ['out']\n",
      "\n",
      "============================================================\n",
      "Dataset: violence_safe\n",
      "============================================================\n",
      "Total items: 1170\n",
      "File types: {'.jpg': 64, '.jpeg': 899, '.png': 198, '.webp': 1, '.gif': 5, '.py': 1}\n",
      "Subdirectories: ['Graphically Violent Images', 'Graphically Safe Images']\n",
      "\n",
      "============================================================\n",
      "Dataset: hate_speech\n",
      "============================================================\n",
      "Total items: 1\n",
      "File types: {'.csv': 1}\n",
      "\n",
      "CSV files found: ['train.csv']\n",
      "\n",
      "First CSV structure (train.csv):\n",
      "  Rows: 24783\n",
      "  Columns: ['count', 'hate_speech_count', 'offensive_language_count', 'neither_count', 'class', 'tweet']\n",
      "\n",
      "First few rows:\n",
      "   count  hate_speech_count  offensive_language_count  neither_count  class  \\\n",
      "0      3                  0                         0              3      2   \n",
      "1      3                  0                         3              0      1   \n",
      "2      3                  0                         3              0      1   \n",
      "\n",
      "                                               tweet  \n",
      "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
      "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
      "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n"
     ]
    }
   ],
   "source": [
    "# Check what we got\n",
    "print(\"\\nExploring downloaded datasets...\\n\")\n",
    "\n",
    "for dataset_name in ['nsfw_detection', 'violence_safe', 'hate_speech']:\n",
    "    dataset_path = Path(f'datasets/raw/{dataset_name}')\n",
    "    \n",
    "    if not dataset_path.exists():\n",
    "        print(f\"⚠️  {dataset_name} not found\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Count files by extension\n",
    "    file_types = {}\n",
    "    all_files = list(dataset_path.rglob('*'))\n",
    "    \n",
    "    for f in all_files:\n",
    "        if f.is_file():\n",
    "            ext = f.suffix.lower()\n",
    "            file_types[ext] = file_types.get(ext, 0) + 1\n",
    "    \n",
    "    print(f\"Total items: {len(all_files)}\")\n",
    "    print(f\"File types: {file_types}\")\n",
    "    \n",
    "    # Look for common patterns\n",
    "    subdirs = [d for d in dataset_path.iterdir() if d.is_dir()]\n",
    "    if subdirs:\n",
    "        print(f\"Subdirectories: {[d.name for d in subdirs[:5]]}\")\n",
    "    \n",
    "    # Check for CSV files\n",
    "    csv_files = list(dataset_path.rglob('*.csv'))\n",
    "    if csv_files:\n",
    "        print(f\"\\nCSV files found: {[f.name for f in csv_files]}\")\n",
    "        # Load first CSV to see structure\n",
    "        df = pd.read_csv(csv_files[0])\n",
    "        print(f\"\\nFirst CSV structure ({csv_files[0].name}):\")\n",
    "        print(f\"  Rows: {len(df)}\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        print(f\"\\nFirst few rows:\")\n",
    "        print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8393579d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Metadata saved to datasets/metadata/download_info.json\n",
      "\n",
      "============================================================\n",
      "FINAL SUMMARY\n",
      "============================================================\n",
      "\n",
      "nsfw_detection:\n",
      "  Images: 27731\n",
      "  Total files: 27741\n",
      "\n",
      "violence_safe:\n",
      "  Images: 1166\n",
      "  Total files: 1170\n",
      "\n",
      "hate_speech:\n",
      "  Images: 0\n",
      "  Total files: 1\n"
     ]
    }
   ],
   "source": [
    "# Save metadata about what we downloaded\n",
    "metadata = {\n",
    "    'datasets': []\n",
    "}\n",
    "\n",
    "for name, kaggle_path in datasets.items():\n",
    "    dataset_path = Path(f'datasets/raw/{name}')\n",
    "    \n",
    "    if dataset_path.exists():\n",
    "        all_files = list(dataset_path.rglob('*'))\n",
    "        image_files = [f for f in all_files if f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif']]\n",
    "        \n",
    "        metadata['datasets'].append({\n",
    "            'name': name,\n",
    "            'kaggle_path': kaggle_path,\n",
    "            'local_path': str(dataset_path),\n",
    "            'total_files': len(all_files),\n",
    "            'image_count': len(image_files),\n",
    "            'downloaded': True\n",
    "        })\n",
    "    else:\n",
    "        metadata['datasets'].append({\n",
    "            'name': name,\n",
    "            'kaggle_path': kaggle_path,\n",
    "            'downloaded': False\n",
    "        })\n",
    "\n",
    "import json\n",
    "with open('datasets/metadata/download_info.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"\\n✅ Metadata saved to datasets/metadata/download_info.json\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for ds in metadata['datasets']:\n",
    "    if ds['downloaded']:\n",
    "        print(f\"\\n{ds['name']}:\")\n",
    "        print(f\"  Images: {ds['image_count']}\")\n",
    "        print(f\"  Total files: {ds['total_files']}\")\n",
    "    else:\n",
    "        print(f\"\\n{ds['name']}: ❌ Not downloaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
